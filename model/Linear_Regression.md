# Linear Regression
## 선형 회귀
- 선형 회귀를 훈련시키는 두 가지 방법
  - 직접 계산할 수 있는 공식을 사용하여 훈련 세트에 가장 잘 맞는 모델 파라미터(훈련 세트에 대해 비용 함수를 최소화하는 모델 파라미터)  
    를 해석적으로 구함
  - 경사 하강법(GD)이라 불리는 반복적인 최적화 방식을 사용하여 모델 파라미터를 조금씩 바꾸면서 비용 함수를 훈련 세트에 대해 최소화시킴 
    결국 위의 방법과 동일한 파라미터로 수렴함
- 선형 회귀는 입력 특성의 가중치(독립변수의 계수)와 편향(절편)이라는 상수를 더해 예측을 만듬
- 선형 회귀 모델 식은 다음과 같음
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?\hat{y}&space;=&space;\theta_{0}&space;&plus;&space;\theta_{1}x_{1}&space;&plus;&space;\theta_{2}x_{2}&space;&plus;&space;...&space;&plus;&space;\theta_{n}x_{n}" /></p>

- yHat은 예측값  
- n은 특성의 수
- x(i)는 i번째 특성값
- theta(j)는 j번째 모델 파라미터(절편은 theta(0))
- 선형 회귀 모형을 훈련시키려면 RMSE를 최소화하는 theta를 찾아야 함

### 정규방정식(normal equation)
- 비용 함수 MSE(theta)를 최소화 하는 함수 계수를 찾기 위한 해석적인 방법이 있음
- 다른 말로 하면 다로 결과를 얻을 수 있는 수학 공식이 존재
- 이를 <b>정규방정식(normal equation)</b>이라고 함
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?\hat\theta&space;=&space;(X^{T}X)^{-1}X^{T}y" /></p>

- theta hat은 비용 함수를 최소화하는 theta 값
- y는 y(1) 부터 y(m)을 포함하는 타깃 벡터

### 경사 하강법
- 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘
- 경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해 나가는 것
- 경사 하강법의 원리는 발바닥이 닿는 바닥의 기울기만 가지고 골짜기를 찾아가는 방법과 유사
- 파라미터 벡터 theta에 대해 비용 함수의 현재 그레이디언트를 계산  
  (그레디언트는 비용 함수의 미분값으로 포괄적으로 사용)
- 그리고 그레디언트가 감소하는 방향으로 진행. 그레디언트가 0이 되면 최솟값에 도달한 것
- theta를 임의의 값을 시작해서(무작위 초기화, random initialization) 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킴
- 학습 스텝의 크기는 비용 함수의 기울기에 비례. 따라서 파라미터가 최솟값에 가까워질수록 스텝 크기가 점진적으로 줄어듬
- 경사 하강법에서 중요한 파라미터는 스텝의 크기로, learning rate 하이퍼 파라미터로 결정