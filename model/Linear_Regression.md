# Linear Regression
## 선형 회귀
- 선형 회귀를 훈련시키는 두 가지 방법
  - 직접 계산할 수 있는 공식을 사용하여 훈련 세트에 가장 잘 맞는 모델 파라미터(훈련 세트에 대해 비용 함수를 최소화하는 모델 파라미터를 해석적으로 구함
  - 경사 하강법(GD)이라 불리는 반복적인 최적화 방식을 사용하여 모델 파라미터를 조금씩 바꾸면서 비용 함수를 훈련 세트에 대해 최소화시킴 
    결국 위의 방법과 동일한 파라미터로 수렴함
- 선형 회귀는 입력 특성의 가중치(독립변수의 계수)와 편향(절편)이라는 상수를 더해 예측을 만듬
- 선형 회귀 모델 식은 다음과 같음
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?\hat{y}&space;=&space;\theta_{0}&space;&plus;&space;\theta_{1}x_{1}&space;&plus;&space;\theta_{2}x_{2}&space;&plus;&space;...&space;&plus;&space;\theta_{n}x_{n}" /></p>

- yHat은 예측값  
- n은 특성의 수
- x(i)는 i번째 특성값
- theta(j)는 j번째 모델 파라미터(절편은 theta(0))
- 선형 회귀 모형을 훈련시키려면 RMSE를 최소화하는 theta를 찾아야 함

### 정규방정식(normal equation)
- 비용 함수 MSE(theta)를 최소화 하는 함수 계수를 찾기 위한 해석적인 방법이 있음
- 다른 말로 하면 바로 결과를 얻을 수 있는 수학 공식이 존재
- 이를 <b>정규방정식(normal equation)</b>이라고 함
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?\hat\theta&space;=&space;(X^{T}X)^{-1}X^{T}y" /></p>

- theta hat은 비용 함수를 최소화하는 theta 값
- y는 y(1) 부터 y(m)을 포함하는 타깃 벡터
- <b>특성의 개수가 늘어날수록 성능 저하가 심하게 일어남</b>
- R에서의 `lm()` 함수는 QR분해를 이용하여 선형 회귀를 계산함


### 경사 하강법
- 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘
- 경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해 나가는 것
- 경사 하강법의 원리는 발바닥이 닿는 바닥의 기울기만 가지고 골짜기를 찾아가는 방법과 유사
- 파라미터 벡터 theta에 대해 비용 함수의 현재 그레이디언트를 계산  
  (그레디언트는 비용 함수의 미분값으로 포괄적으로 사용)
- 그리고 그레디언트가 감소하는 방향으로 진행. 그레디언트가 0이 되면 최솟값에 도달한 것
- theta를 임의의 값을 시작해서(무작위 초기화, random initialization) 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킴
- 학습 스텝의 크기는 비용 함수의 기울기에 비례. 따라서 파라미터가 최솟값에 가까워질수록 스텝 크기가 점진적으로 줄어듬
- 경사 하강법에서 중요한 파라미터는 스텝의 크기로, learning rate 하이퍼 파라미터로 결정
- 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로, 시간이 오래 걸림
- 학습률이 너무 크면 골짜기를 가로질러 반대편으로 건너뛰게 되어 이전보다 더 높은 곳으로 올라가게 됨
- 모든 비용함수가 2차 볼록 함수 형태는 아님. 따라서 전역 최솟값(global minimum)이 아닌 지역 최솟값(local minimum)에 수렴할 수 있음
- 선형 회귀를 위한 MSE 비용 함수는 항상 2차 볼록함수이기 때문에 어디서 시작하던 전역 최솟값만 있고 기울기가 갑자기 변하지 않음
- 이 두 사실로부터 경사 하강법이 전역 최솟값에 가깝게 접근할 수 있음을 보장함
- 사실 비용 함수는 그릇 모양을 하고 있지만 특성들의 스케일이 매우 다르면 길쭉한 모양일 수 있음
- 즉 스케일이 다르면 전역 최솟값에 도달하는 시간이 더 오래걸리므로, scale을 조정해 주어야 함
- 모델 훈련은 비용 함수를 최소화하는 모델 파라미터를 찾는 일이고, 이를 모델의 <b>파라미터 공간</b>에서 찾는다고 말함

### 배치 경사 하강법
- 경사 하강법을 구현하려면 각 모델 파라미터 theta(j)에 대해 비용 함수의 그레디언트를 계산해야함
- 다시 말해 theta(j)가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산해야함. 이를 <b>편도함수(partial derivative)</b>라고 함  
이는 "동쪽을 바라봤을 때 발밑에 느껴지는 산의 기울기는 얼마인가?"와 같은 질문 
- 이러한 같은 질문을 모든 차원에서 행함
- 다음의 식은 파라미터 theta(j)에 대한 비용 함수의 편도함수임
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;}{\partial\theta_{j}}&space;MSE(\theta)&space;=&space;\frac{2}{m}&space;\sum_{i&space;=&space;1}^{m}(\theta^{T}x^{i}&space;-&space;y^{i})&space;(x_{j})^{i}" /></p>

- 위의 식을 이용해 편도 함수를 각각 계산할 수도 있지만, 선형 대수로 한꺼번에도 계산 가능
- 수식에 theta가 있기 때문에 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산함
- 그래서 이 알고리즘을 <b>배치 경사 하강법(batch gradient descent)</b>라고 함
- 매 스텝에서 훈련 데이터 전체를 사용함(전체 경사 하강법이 더 적절한 이름 같음)
- 이런 이유로 매우 큰 훈련 세트에서는 아주 느림
- <b>그러나 경사 하강법은 특성 수에는 민감하지 않음</b>
- <b>위로 향하는 그레디언트 벡터가 구해지면, 반대 방향인 아래로 가야 함</b>
- 즉 theta에서 편도 함수의 값을 빼야한다는 의미. 여기서 학습률도 사용됨
- 다음은 경사 하강법의 스텝 식임
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?\theta^{next&space;step}&space;=&space;\theta&space;-&space;\eta&space;\bigtriangledown_{\theta}&space;MSE(\theta)" /></p>

- 적절한 반복 휫수를 찾으려면, 반복 횟수를 아주 크게 지정하고 그레디언트 벡터가 아주 작아지면 벡터의 노름이 허용 오차보다 작아지면 알고리즘을 중지시키는 방법이 있음
- 수렴율  
  - 비용함수의 모양에 따라 달라지겠지만 허용 오차 범위 안에서 최적의 솔루션에 도달하기 위해서는 O(1/허용오차)의 반복이 걸릴 수 있음
  - <b>허용 오차를 1/10으로 줄이면 알고리즘의 반복은 10배 늘어남</b>

### 확률적 경사 하강법
- 배치 경사 하강법의 가장 큰 문제는 매 스탭에서 전체 훈련 세트를 이용해 그레디언트를 계산 한다는 사실
- 훈련 세트가 커지면 매우 느려지게 됨
- <b>확률적 경사 하강법</b>은 매 스텝에서 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그레디언트를 계산함
- 매 반복에서 다뤄야 할 데이터가 매우 적기 때문에 한 번에 하나의 샘플을 처리하면 알고리즘이 확실히 훨씬 빠름
- 반면 확률적(무작위)이기 때문에 이 알고리즘은 배치 경사 하강법보다 훨씬 불안정 함
- 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 요동치면서 평균적으로 감소함
- 즉, 시간이 지나면 최솟값에 매우 근접하겠지만 요동이 지속되면서 최솟값에 안착하지 못할 것임
- 알고리즘이 멈출 때 좋은 파라미터가 구해지겠지만 최적치는 아님
- 무작위성은 지역 최솟값에서 탈출시켜줘서 좋지만 알고리즘을 전역 최솟값에 다다르지 못하게 한다는 점에서 좋지 않음
- 딜레마를 해결하는 한 가지 방법은 학습률을 점진적으로 감소시키는 것
- 시작할 때는 학습률을 크게하지만 점차 작게 줄여서 알고리즘이 전역 최솟값에 도달하게 함
- 일반적으로 한 반복에서 m번(훈련 세트에 있는 샘플 수) 되풀이 되고, 이때 각 반복을 epoch라고 함
- 샘플은 무작위로 선택되기 때문에 한 샘플은 2번 이상 선택 될 수도 있고, 아예 선택받지 못할 수도 있음
- 따라서 알고리즘이 epoch마다 모든 샘플을 사용하게 하려면 훈련 세트를 섞은 후 차례대로 하나씩 선택하고 다음 에포크에서 다시 섞는 식의 방법을 사용할 수 있음 <b>그러나 이렇게 하면 보통 더 늦게 수렴됨</b>

### 미니배치 경사 하강법
- 전체 훈련 세트나 하나의 샘플을 기반으로 그레디언트를 계산하는 것이 아니라  
  미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레디언트를 계산함
- 확률적 경사 하강법에 비해서 장점은 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 얻는 성능 향상

## 다항 회귀
- 비선형 데이터를 학습하는 데 선형 모델을 사용할 수 있음
- 이렇게 하는 간단한 방법은 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련시키는 것. 이런 기법을 <b>다항 회귀</b>라고 함
- 특성이 여러 개일 때 다항 회귀는 특성 사이의 관계를 찾을 수 있음(교호작용)
- 특성 a, b가 있을때 degree = 2이면, a^2, b^2, ab, a, b도 특성으로 추가함

### 학습 곡선
- 고차 다항 회귀는 보통의 선형 회귀보다 훨씬 더 훈련 데이터에 잘 맞추려고 할 것임
- 보통 n차 다항 회귀는 n이 커질수록 점점 더 과대적합 될 확률이 높은데, 이 때 적절한 n의 값은 어떻게 찾을 수 있을까?
  - 모델의 일반화 성능을 추정하기 위해 교차 검증을 이용하여 평가  
    훈련 데이터의 성능이 좋은데, test Dataset의 성능이 좋지 않다면 과대적합 된 것  
    --> n의 값을 낮추어야 함
  - 만약 train/test Dataset에 대한 성능이 둘 다 좋지 못하다면 과소적합 된 것
  - <b>학습 곡선</b>을 살펴보는 것도 도움이 됨  
    이 그래프는 훈련 세트의 크기(x축)에 따른 모델 성능의 크기의 그래프로 나타냄  
    만약 검증 데이터와 훈련 데이터의 평균 성능이 낮다면 과소 적합 되어 있다는 의미
- 모델이 훈련 데이터에 과소적합 되어 있다면 훈련 데이터를 추가한다고 할지라도 큰 효과가 없음  
  더 복잡한 모델로 교체해야함
- 과대적합 모델을 개선하는 한 가지 방법은 검증 오차가 훈련 오차에 근접할 때까지 더 많은 훈련 데이터를 추가하는 것

#### 편향(bias)/분산(variance) 트레이드 오프
- 통계학과 머신러닝에서 나온 중요한 이론 하나는 모델의 일반화 오차는 세가지 다른 종류의 오차의 합으로 표현될 수 있다는 사실
  - 편향(bias)  
    잘못된 가정으로 인한 것. 예를 들어 데이터는 실제 2차인데 선형함수로 가정하는 경우,  
    편향이 큰 모델은 훈련 데이터에 과소적합되기 쉬움
  - 분산(variance)  
    분산은 훈련 데이터에 있는 작은 변동이 모델에 과도하게 민감해서 나타남  
    자유도가 높은 모델(예를 들어 고차 다항 회귀)이 높은 분산을 가지기 쉬워 훈련 데이터에 과대적합되는 경향이 있음 
  - 줄일 수 없는 오차  
    데이터 자체의 잡음 때문에 발생. 이 오차를 줄이려면 데이터 자체의 이상치 등을 제거해야함

## Ridge 회귀
- 규제가 추가된 회귀 버전
- 가중치 벡터 L2 노름을 사용
- 규제를 하는 이유는 모델의 오버피팅을 줄이기 위하여 규제를 수행
- 규제항 (theta)^2 의 합이 비용 함수에 추가됨
- 이는 학습 알고리즘을 데이터에 맞추는 것 뿐만 아니라 모델의 가중치가 가능한 한 작게 유지되도록 노력함
- <b>규제항은 학습하는 동안에만 비용 함수에 추가되고, 실제 모델 평가에는 규제항은 제외됨</b>
- 일반적으로 훈련하는 동안 사용되는 비용 함수와 테스트에서 사용되는 성능 지표는 다른데,  
  규제를 떠나서 이들이 다른 이유는 비용 함수는 미분 가능해야하고, 성능 지표는 최종 목표에 가능한 한 가까워야 함
- 하이퍼파라미터 alpha는 모델을 얼마나 규제할지를 결정함  
  alpha가 0이면 릿지 회귀는 선형 회귀와 같아짐
- alpha가 매우 커지면, 가중치가 거의 0에 가까워지고,  데이터의 평균선과 비슷해짐
- 다음은 릿지 회귀의 비용함수임
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?J(\theta)&space;=&space;MSE(\theta)&space;&plus;&space;\alpha\frac{1}{2}\sum_{i&space;=&space;1}^{n}\theta^{2}" /></p>

- 편향 theta(0)는 규제되지 않음
- 릿지 회귀는 입력 특성의 스케일에 민감하기 때문에 수행하기 전에 데이터의 스케일을 맞추는 것이 중요
- alpha가 커질수록 직선에 가까워지며, 이는 편향은 커지고 분산은 줄어들게 됨
- 릿지 회귀도 마찬가지로 정규방정식을 사용할 수도 있고, 경사 하강법을 사용할 수도 있음

## Lasso 회귀
- Least absolute shrinkage and selection operator(LASSO)
- 가중치 벡터 L1 노름을 사용함
- 라쏘 회귀의 비용함수는 다음과 같음
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?J(\theta)&space;=&space;MSE(\theta)&space;&plus;&space;\alpha\frac{1}{2}\sum_{i&space;=&space;1}^{n}|\theta|" /></p>

- 라쏘 회귀의 중요한 특징은 덜 중요한 특성의 가중치를 제거하려고 하는 것(가중치의 값이 0이 됨)
- 라쏘 회귀는 자동으로 특성 선택을 하고 희소 모델(sparse model)을 만듬
- 라쏘를 사용할 때 경사 하강법이 최적점 근처에서 진동하는 것을 막으려면 훈련하는 동안 점진적으로 학습률을 감소시켜야 함

## elasticNet 회귀
- 엘라스틱넷은 릿지 회귀와 라쏘 회귀를 절충한 모델
- 규제항은 릿지와 라소의 규제항을 단순히 더해서 사용하며, 혼합 정도는 혼합 비율 r을 사용해 조절
- r = 0이면 엘라스틱넷은 릿지 회귀와 같고, r = 1이면 lasso 회귀와 같음
- 엘라스틱넷의 비용함수는 다음과 같음
<p align = 'center'><img src="https://latex.codecogs.com/gif.latex?J(\theta)&space;=&space;MSE(\theta)&space;&plus;&space;r\alpha\sum_{i&space;=&space;1}^{n}|\theta|&space;&plus;&space;\frac{1-&space;r}{2}\alpha\sum_{i&space;=&space;1}^{n}\theta^{2}" /></p>

- 그렇다면 보통의 선형 회귀, 릿지, 라쏘, 엘라스틱넷을 언제 사용해야 할까요?  
  --> 적어도 규제가 약간은 있는 것이 대부분의 경우에 좋으므로 일반적으로 평범한 선형 회귀는 피해야 함  
- 릿지가 기본이 되지만 쓰이는 특성이 몇 개뿐이라고 의심되면 라쏘나 엘라스틱넷이 더 좋음
- 라쏘는 특성 수가 샘플 수(n)가 보다 많으면 최대 n개의 특성을 선택함
- 또한 여러 특성이 강하게 연관되어 있으면 이들 중 임의의 특성 하나를 선택함 
- 적절한 모델 학습 반복 횟수를 지정하기 위하여 early stopping을 하는 것이 좋음
- 확률 경사 하강법이나 미니배치 경사 하강법은 곡선이 그리 매끄럽지 않아 최솟값에 도달했는지 확인하기 어려울 수 있음. 한 가지 해결책은 일정 시간 동안 최솟값보다 클 때 학습을 멈추고 최솟값일 때의 모델 파라미터로 되돌리는 것