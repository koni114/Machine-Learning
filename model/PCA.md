## 차원 축소(Dimension Shrink)

## 차원 축소
- 많은 경우 ML 문제는 훈련 샘플 각각이 수천, 수백만 개의 특성을 가지고 있음
- 이런 많은 특성은 훈련을 느리게 할 뿐만 아니라 좋은 솔루션을 찾기 어렵게 만듬
- 이런 문제를 종종 <b>차원의 저주(cuse of dimensionality)</b>라고 함
- 차원을 축소시키면 일부 정보가 유실되며, 훈련 속도는 빨라질 수 있지만 시스템의 성능이  
  조금 나빠질 수 있음
- 그러므로 차원 축소를 하기 전에 훈련이 너무 느린지, 원본 데이터로 시스템을 훈련해봐야 함
- 어떤 경우에는 훈련 데이터의 차원을 축소시키면 잡음이나 불필요한 세부사항을 걸러내므로  
  성능을 높일 수 있음
- 차원 축소는 훈련 속도를 높이는 것과 데이터 시각화에도 아주 유용함
- 차원 수를 둘로 줄이면 고차원 훈련 세트를 하나의 압축된 그래프로 그릴 수 있고,  
  군집 같은 시각적인 패턴을 감지해 중요한 통찰을 얻는 경우가 많이 있음

## 차원의 저주
- 우리는 3차원 세계에서 살고 있어 고차원 공간을 직관적으로 상상하기는 어려움
- 1,000차원 공간에서 휘어져 있는 200차원의 타원체는 고사하고 기본적인 4차원 초입방체 조차도  
  머릿속에 그리기가 어려움
- 단위 면적에서 임의의 두 점을 선택하면 두 점 사이의 거리는 평균적으로 대략 0.52가 됨
- 3차원 큐브에서 임의의 두 점을 선택하면 평균 거리는 대략 0.66
- 1,000,000 차원의 초입방체에서 두 점을 무작위로 선택하면 평균 거리는 428.25
- 고차원은 많은 공간을 가지고 있기 때문
- <b>이로 인해 고차원 데이터셋은 매우 희박할 위험이 있음</b> 
- 즉 대부분의 훈련 데이터가 서로 멀리 떨어져 있음. 이는 새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다는 의미
- 이 경우 예측을 위해서 훨씬 많은 외삽(extrapolation)을 해야하기 떄문에 저차원일 때보다 예측이 더 불안정함
- 간단히 말해 훈련 세트의 차원이 클수록 과대적합의 위험이 커짐
- 이론적으로 차원의 저주를 해결하는 해결책 하나는 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 것
- 불행하게도 일정 밀도에 도달하기 위해 필요한 샘플 수는 차원 수가 커짐에 따라 기하급수적으로 늘어남

## 차원 축소를 위한 접근 방법
- 차원을 감소시키는 두 가지 주요한 접근법인 투영(projection)과 매니폴드(manifold) 학습을 알아보자

### 투영(projection)
- 대부분의 실전 문제는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않음
- 많은 특성들은 거의 변화가 없고, 다른 특성들은 서로 강하게 연관되어 있음
- 결과적으로 모든 훈련 샘플이 고차원 공간 안의 <b>저차원 부분 공간(subspace)</b>에 놓여 있음

### 매니폴드 학습
- 스위스 롤은 2D 매니폴드의 한 예
- 간단히 말해 2D 매니폴드는 고차원 공간에서 휘어지거나 뒤틀린 2D 모양
- 더 일반적으로 d차원 매니폴드는 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부(d < n)
- 스위스 롤의 경우에는 d = 2이고, n = 3임
- 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링하는 식으로 작동  
  이를 <b>매니폴드 학습</b>이라고 함
- 이는 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 <b>매니폴드 가정(manifold assumption)</b> 또는 매니폴드 가설에 근거
- 여기서 MNIST로 생각해보자. 전체 손글씨 숫자 이미지는 어느 정도 비슷한 면이 있는데,  
  선으로 연결되어 있고 경계는 흰색이고 어느 정도 중앙에 위치함
- 무작위로 생성된 이미지라면 그중 아주 적은 일부만 손글씨 숫자처럼 보일 것임 
- 다시 말해 숫자 이미지를 만들 때 가능한 자유도는 아무 이미지나 생성할 때의 자유도보다 훨씬 낮음
- 이런 제약은 데이터셋을 저차원의 매니폴드로 압축할 수 있도록 도와줌
- 매니폴드 가정은 이 저차원의 매니폴드 공간에 표현되면 더 간단해 질 것 이라는 가정을 동반  
  하지만 항상 이 가정이 맞는 것은 아님.
- 당연히 차원을 늘렸을 때 더 간단하게 데이터를 분할 시킬 수 있는 경우가 있음!
- 결과적으로 무조건 차원 수를 낮춘다고 성능 향상이 되는 것은 아님

## PCA(principle component Analysis)
- 주성분 
  