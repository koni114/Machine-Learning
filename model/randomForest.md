# randomForest
## randomForest 기법 설명
- 트리 기반의 대표적인 앙상블 모형
- 랜덤포레스트는 같은 데이터에 여러개의 의사결정모형을 생성하여 학습 성능을 높이는 앙상블 기법의 모형
- 동일한 데이터로부터 복원 추출(부트스트랩 샘플링)을 통해 n개 이상의 학습 데이터를 편성하고 각각의 데이터마다 의사결정모형을 생성해 결과를 취합하는 방식
- 여기서 각각의 나무들은 일부 변수만을 취해 학습하게 됨
- 개별 트리들이 데이터를 바라보는 관점을 다르게해 다양성을 높이고자 하는 시도
- <b>로테이션 포레스트</b>는 데이터에 PCA를 적용해 데이터 축을 회전한 후 학습한다는 점만 제외하고 나머지는 동일함
- 이러한 bagging 기반의 앙상블 모형이 단일 모형보다 더 좋은 성능을 나타내는 이유,  
  --> 큰 수의 법칙 : 동전을 던졌을 때 앞면이 뒷면보다 더 1% 더 높은 확률로 나올 때, 동전을 많이 던지면 던질수록 51%가 나올확률이 점점 커지는 것으로 설명할 수 있음
- 이러한 앙상블은 서로 독립적일 때 최고의 성능을 발휘함
- 서로 오차에 대해 관련이 없는 다양한 분류기를 앙상블 시키면 좋은 모형이 나올 확률이 높아짐
- 일반적으로 soft-voting 방식(prob 값을 평균)이 hard-voting 방식보다 성능이 좋음  
  why? 확률이 높은 비율에 좀 더 비중을 두는 개념이기 때문
- 개별 예측기는 단일 예측 의사결정모형보다 훨씬 더 편향되어 있지만, 여러개의 개별 예측기를 평균 또는 최빈값을 계산하면 편향과 분산 모두 감소
- 일반적으로 단일 예측 모형으로 예측한 값과 앙상블 결과 값과 비교했을 때 편향은 비슷하나 분산은 감소함
- bootstrapping은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 bagging이 pasting보다 편향이 조금 더 높음
- 하지만 다양성을 추가한다는 것은 예측기들의 상관관계를 줄이므로 앙상블의 분산을 감소시킴
- 일반적으로 부트스트랩 방식을 사용하면 한 데이터 샘플당 63%의 데이터가 포함됨.  
  부트스트랩 방식을 이용해서 데이터의 개수 M개 만큼 추출하면, 그정도의 비율이 계산됨  
  (1 - 1/m)^m 에 로그를 취하고 로피탈을 계산하면, 1- e(-1) = 0.63
- 훈련 특성과 샘플을 모두 사용하는 방식을 <b>랜덤 패치</b> 방식(random patch method)
- 샘플을 모두 사용하지만 훈련 특성을 일부분만 사용하는 방식을 <b>서브스페이스 방식</b>이라고 함
- 특성 샘플링은 bias를 증가시키고 variance를 낮춤
- 나머지 37%(OOB, out-of-bag)을 가지고 모델 평가를 수행할 수 있음

## 특성 중요도
- 랜덤포레스트는 비교적 특성중요도를 알아내기가 쉬운데, 어떤 특성을 사용한 노드가 일반적으로 불순도를 얼마만큼 낮추는지를 변수 중요도로 환산
- 더 정확히 말하면 가중치 평균이며 각 노드의 가중치는 연관된 훈련 샘플 수와 같음

## R 기반 구현
- R에서 의사결정나무를 수행하는 패키지는 `rpart`임
- 의사결정나무는 범주(classification)을 예측할 경우 범주형, 연속형 값을 맞추는 회귀(Regression)모두 가능함
- 분류를 하고 싶다면, 아래 예제에서 `type = 'class'`를, 회귀를 하고 싶다면 `type = anova`를 지정하면 됨
- 이렇게 명시적으로 적어주지 않아도 Y 값의 타입이 범주형이면 분류를, 연속형이면 예측을 수행
~~~r
library(rpart)
Fit1 <- rpart(fomula, data, type=“anova”) # regression
Fit2 <- rpart(fomula, data, type=“class”) # classification
~~~
- 랜덤포레스트와 관련된 패키지명은 말 그대로 `randomForest`임
- rpart와 마찬가지로 Y의 자료형이 factor이면 분류, numeric이면 자동으로 분류와 회귀를 수행
- `mtry`  : 샘플링 할 시 랜덤으로 선택하는 독립 변수의 개수
- `ntree` : 앙상블을 수행할 의사결정나무의 개수
~~~r
library(randomForest)
Fit <- randomForest(formula, data, ntree, mtry, ...)
~~~

## 용어 정리
- bagging
  - 같은 알고리즘 모형에 훈련 데이터셋을 무작위로 구성(복원)하여 앙상블 하는 것
- pasting
  - 같은 알고리즘 모형에 훈련 데이터셋을 무작위로 구성(비복원)하여 앙상블 하는 것